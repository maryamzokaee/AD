{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"yVd7YbxbuwkB","outputId":"318ef5d7-d477-425a-9649-b6751a0f11a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["/code/interpretable-han-for-document-classification-with-keras\n"]}],"source":"cd /code/interpretable-han-for-document-classification-with-keras"},{"cell_type":"code","execution_count":2,"metadata":{"id":"xG3ThTWCf4v8","outputId":"f8e8e606-d9fc-45ea-fad3-d1f851f07109"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pip\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/c2/e06851e8cc28dcad7c155f4753da8833ac06a5c704c109313b8d5a62968a/pip-23.2.1-py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 18.5MB/s eta 0:00:01\n","\u001b[?25hInstalling collected packages: pip\n","  Found existing installation: pip 19.1.1\n","    Uninstalling pip-19.1.1:\n","      Successfully uninstalled pip-19.1.1\n","Successfully installed pip-23.2.1\n"]}],"source":"!pip install --upgrade pip"},{"cell_type":"code","execution_count":3,"metadata":{"id":"J9pTm6y4f4v9","outputId":"073b0b0b-3123-4284-dd1f-55576e85e1e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting numpy==1.18\n","  Downloading numpy-1.18.0-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.19.2\n","    Uninstalling numpy-1.19.2:\n","      Successfully uninstalled numpy-1.19.2\n","Successfully installed numpy-1.18.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":"!pip install numpy==1.18"},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z7CC1CQSJoFw","outputId":"0c32d940-30ee-4b70-d128-edda5442ca95"},"outputs":[{"name":"stdout","output_type":"stream","text":["running install\n","running bdist_egg\n","running egg_info\n","writing hierachical_attention_network_for_document_classification.egg-info/PKG-INFO\n","writing dependency_links to hierachical_attention_network_for_document_classification.egg-info/dependency_links.txt\n","writing requirements to hierachical_attention_network_for_document_classification.egg-info/requires.txt\n","writing top-level names to hierachical_attention_network_for_document_classification.egg-info/top_level.txt\n","reading manifest file 'hierachical_attention_network_for_document_classification.egg-info/SOURCES.txt'\n","writing manifest file 'hierachical_attention_network_for_document_classification.egg-info/SOURCES.txt'\n","installing library code to build/bdist.linux-x86_64/egg\n","running install_lib\n","running build_py\n","creating build/bdist.linux-x86_64\n","creating build/bdist.linux-x86_64/egg\n","creating build/bdist.linux-x86_64/egg/han\n","copying build/lib/han/selfattention_HAN_3.py -> build/bdist.linux-x86_64/egg/han\n","copying build/lib/han/selfattention_han_3_last.py -> build/bdist.linux-x86_64/egg/han\n","copying build/lib/han/AoS2.py -> build/bdist.linux-x86_64/egg/han\n","copying build/lib/han/model.py -> build/bdist.linux-x86_64/egg/han\n","copying build/lib/han/utils.py -> build/bdist.linux-x86_64/egg/han\n","copying build/lib/han/attention.py -> build/bdist.linux-x86_64/egg/han\n","copying build/lib/han/__init__.py -> build/bdist.linux-x86_64/egg/han\n","copying build/lib/han/HBRNN.py -> build/bdist.linux-x86_64/egg/han\n","byte-compiling build/bdist.linux-x86_64/egg/han/selfattention_HAN_3.py to selfattention_HAN_3.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/han/selfattention_han_3_last.py to selfattention_han_3_last.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/han/AoS2.py to AoS2.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/han/model.py to model.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/han/utils.py to utils.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/han/attention.py to attention.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/han/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/han/HBRNN.py to HBRNN.cpython-37.pyc\n","creating build/bdist.linux-x86_64/egg/EGG-INFO\n","copying hierachical_attention_network_for_document_classification.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying hierachical_attention_network_for_document_classification.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying hierachical_attention_network_for_document_classification.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying hierachical_attention_network_for_document_classification.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying hierachical_attention_network_for_document_classification.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","zip_safe flag not set; analyzing archive contents...\n","creating 'dist/hierachical_attention_network_for_document_classification-0.1.0-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n","removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n","Processing hierachical_attention_network_for_document_classification-0.1.0-py3.7.egg\n","Copying hierachical_attention_network_for_document_classification-0.1.0-py3.7.egg to /opt/conda/lib/python3.7/site-packages\n","Adding hierachical-attention-network-for-document-classification 0.1.0 to easy-install.pth file\n","\n","Installed /opt/conda/lib/python3.7/site-packages/hierachical_attention_network_for_document_classification-0.1.0-py3.7.egg\n","Processing dependencies for hierachical-attention-network-for-document-classification==0.1.0\n","Searching for pandas==1.3.5\n","Best match: pandas 1.3.5\n","Adding pandas 1.3.5 to easy-install.pth file\n","\n","Using /opt/conda/lib/python3.7/site-packages\n","Searching for Keras==2.4.3\n","Best match: Keras 2.4.3\n","Adding Keras 2.4.3 to easy-install.pth file\n","\n","Using /opt/conda/lib/python3.7/site-packages\n","Searching for numpy==1.18.0\n","Best match: numpy 1.18.0\n","Adding numpy 1.18.0 to easy-install.pth file\n","Installing f2py script to /opt/conda/bin\n","Installing f2py3 script to /opt/conda/bin\n","Installing f2py3.7 script to /opt/conda/bin\n","\n","Using /opt/conda/lib/python3.7/site-packages\n","Searching for pytz==2022.7\n","Best match: pytz 2022.7\n","Adding pytz 2022.7 to easy-install.pth file\n","\n","Using /opt/conda/lib/python3.7/site-packages\n","Searching for python-dateutil==2.8.2\n","Best match: python-dateutil 2.8.2\n","Adding python-dateutil 2.8.2 to easy-install.pth file\n","\n","Using /opt/conda/lib/python3.7/site-packages\n","Searching for scipy==1.4.1\n","Best match: scipy 1.4.1\n","Adding scipy 1.4.1 to easy-install.pth file\n","\n","Using /opt/conda/lib/python3.7/site-packages\n","Searching for h5py==2.10.0\n","Best match: h5py 2.10.0\n","Adding h5py 2.10.0 to easy-install.pth file\n","\n","Using /opt/conda/lib/python3.7/site-packages\n","Searching for PyYAML==5.3.1\n","Best match: PyYAML 5.3.1\n","Adding PyYAML 5.3.1 to easy-install.pth file\n","\n","Using /opt/conda/lib/python3.7/site-packages\n","Searching for six==1.12.0\n","Best match: six 1.12.0\n","Adding six 1.12.0 to easy-install.pth file\n","\n","Using /opt/conda/lib/python3.7/site-packages\n","Finished processing dependencies for hierachical-attention-network-for-document-classification==0.1.0\n"]}],"source":"!python /code/interpretable-han-for-document-classification-with-keras/setup.py install"},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ThkIBuP54Gt7","outputId":"a8a99e35-6038-4ae0-838a-14d84d7a6239"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting keras-self-attention\n","  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from keras-self-attention) (1.18.0)\n","Building wheels for collected packages: keras-self-attention\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=b1340de2b0785427180f69e0ff4f12055a20acbd735c9f655dc77ac11c12e3a8\n","  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n","Successfully built keras-self-attention\n","Installing collected packages: keras-self-attention\n","Successfully installed keras-self-attention-0.51.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"name":"stdout","output_type":"stream","text":["2023-09-18 14:27:58,675 - default - INFO - Pre-processsing data.\n","2023-09-18 14:27:58,684 - default - INFO - Tokenization.\n","2023-09-18 14:27:58,892 - default - INFO - Creating embedding matrix using pre-trained GloVe vectors.\n"]}],"source":"\"\"\"\nThis code is for 2 level Hierarchichal Attention model\n\"\"\"\n\nimport re\nimport numpy as np\nimport pandas as pd\nimport logging\nimport sys\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils import to_categorical\nfrom nltk.tokenize import sent_tokenize\nfrom sklearn.model_selection import train_test_split\n!pip install keras-self-attention\nfrom han.AoS2 import HAN\nfrom keras_self_attention import SeqSelfAttention\nimport nltk\nnltk.download('punkt')\n\n# Create a logger to provide info on the state of the\n# script\nstdout = logging.StreamHandler(sys.stdout)\nstdout.setFormatter(logging.Formatter(\n    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n))\nlogger = logging.getLogger('default')\nlogger.setLevel(logging.INFO)\nlogger.addHandler(stdout)\n\n\nmax_doc_num = 5\nmax_sent_length = 100\nmax_sent_num = 25\nMAX_VOC_SIZE = 20000\nGLOVE_DIM = 200\nTEST_SPLIT = 0.1\n\nSpecial_value=0\n\n\n#####################################################\n# Pre processing                                    #\n#####################################################\nlogger.info(\"Pre-processsing data.\")\n\ndata = pd.read_csv('/data/Sample_data_Other.csv', encoding='latin1')\n\n\n   \ndf=data.iloc[:,1:max_doc_num+1]\nreviews = data.iloc[:,1:max_doc_num+1].values\ntarget = data['sentiment'].values\n\ndf['concat'] = pd.Series(df.fillna('').values.tolist()).map(lambda x: ''.join(map(str,x)))\n#del data\n\n\n#####################################################\n# Tokenization                                      #\n#####################################################\nlogger.info(\"Tokenization.\")\n\n# Build a Keras Tokenizer that can encode every token\nword_tokenizer = Tokenizer(num_words=MAX_VOC_SIZE)\nword_tokenizer.fit_on_texts(df['concat'])\n\n# Construct the input matrix. This should be a nd-array of\n# shape (n_samples, max_sent_num, max_sent_length).\n# We zero-pad this matrix (this does not influence\n# any predictions due to the attention mechanism.\n\n\n#X = np.zeros((len(reviews), max_sent_num, max_sent_length), dtype='int32')\nX = np.zeros((len(data), max_sent_num, max_sent_length), dtype='int32')\n\n\n\n\nfor i, review in enumerate(reviews):\n      if not pd.isnull(reviews[i]):\n        #word_tokenizer.fit_on_texts(review[j])\n        sentences = sent_tokenize(reviews[i][0])\n        tokenized_sentences = word_tokenizer.texts_to_sequences(\n          sentences\n        )\n        tokenized_sentences = pad_sequences(\n            tokenized_sentences, maxlen=max_sent_length\n        )\n\n        pad_size = max_sent_num - tokenized_sentences.shape[0]\n\n        if pad_size < 0:\n            tokenized_sentences = tokenized_sentences[0:max_sent_num]\n        else:\n            tokenized_sentences = np.pad(\n                tokenized_sentences, ((0,pad_size),(0,0)),\n                mode='constant', constant_values= Special_value\n            )\n\n        X[i] = tokenized_sentences[None, ...]\n      else:\n        \n        X[i]=np.nan\n\n\n\n# Transform the labels into a format Keras can handle\ny = to_categorical(target)\n\n\n\n\n#####################################################\n# Word Embeddings                                   #\n#####################################################\nlogger.info(\n    \"Creating embedding matrix using pre-trained GloVe vectors.\"\n)\n\n# Now, we need to build the embedding matrix. For this we use\n# a pretrained (on the wikipedia corpus) 100-dimensional GloVe\n# model.\n\n# Load the embeddings from a file\nembeddings = {}\nwith open('/data/glove.6B.%dd.txt' % GLOVE_DIM, encoding='utf-8') as file:\n    for line in file:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n\n        embeddings[word] = coefs\n\n# Initialize a matrix to hold the word embeddings\nembedding_matrix = np.random.random(\n    (len(word_tokenizer.word_index) + 1, GLOVE_DIM)\n)\n\n# Let the padded indices map to zero-vectors. This will\n# prevent the padding from influencing the results\nembedding_matrix[0] = 0\n\n# Loop though all the words in the word_index and where possible\n# replace the random initalization with the GloVe vector.\nfor word, index in word_tokenizer.word_index.items():\n    embedding_vector = embeddings.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\n\n"},{"cell_type":"code","execution_count":6,"metadata":{"id":"WX4BthEBr5P0","outputId":"e51b78fa-4800-410e-fa06-760b4973df2e"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-09-18 14:28:19.083592: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-09-18 14:28:19.108272: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499995000 Hz\n","2023-09-18 14:28:19.108788: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56462b139df0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2023-09-18 14:28:19.108817: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"]}],"source":"h_model = HAN(embedding_matrix, max_sent_length=100, max_sent_num=25, max_doc_num=5, word_embed_dim=100, sent_embed_dim=100, doc_embed_dim=100)\n"},{"cell_type":"code","execution_count":7,"metadata":{"id":"uUMVbCFvdiTM"},"outputs":[],"source":"checkpoint_path='/code/model.{epoch:02d}-{val_loss:.2f}.hdf5'"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NyiIKRJQaJ6z","outputId":"42c98d84-5d34-45f5-ada9-cc18fd794d74"},"outputs":[{"name":"stdout","output_type":"stream","text":["fold: 1\n","Epoch 1/20\n"]},{"name":"stderr","output_type":"stream","text":["2023-09-18 14:29:07.564271: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 640000000 exceeds 10% of free system memory.\n","2023-09-18 14:29:08.135046: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 640000000 exceeds 10% of free system memory.\n"]},{"name":"stdout","output_type":"stream","text":["1/8 [==>...........................] - ETA: 0s - loss: 0.6943 - acc: 0.4500"]},{"name":"stderr","output_type":"stream","text":["2023-09-18 14:29:08.876727: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 640000000 exceeds 10% of free system memory.\n","2023-09-18 14:29:09.343844: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 640000000 exceeds 10% of free system memory.\n"]},{"name":"stdout","output_type":"stream","text":["2/8 [======>.......................] - ETA: 3s - loss: 0.6928 - acc: 0.5250"]},{"name":"stderr","output_type":"stream","text":["2023-09-18 14:29:09.905580: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 640000000 exceeds 10% of free system memory.\n"]},{"name":"stdout","output_type":"stream","text":["8/8 [==============================] - ETA: 0s - loss: 0.6934 - acc: 0.5000\n","Epoch 00001: val_loss improved from inf to 0.69320, saving model to /code/model.01-0.69.hdf5\n","8/8 [==============================] - 9s 1s/step - loss: 0.6934 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.5000\n","Epoch 2/20\n","3/8 [==========>...................] - ETA: 3s - loss: 0.6933 - acc: 0.5167"]}],"source":"#####K-fold Cross Validation#####\nfrom sklearn.model_selection import StratifiedKFold\nimport tensorflow as tf\nTEST_SPLIT=0.1\n\nX_train, X_test, y_train, y_test= train_test_split(X, target, test_size=TEST_SPLIT, random_state=0, stratify=y)\n\n\ny_test=to_categorical(y_test)\n\nkFold = StratifiedKFold(n_splits=10)\nscores = []\nidx = 0\nfor train, val in kFold.split(X_train, y_train):\n  idx = idx+1\n  print(\"fold:\", idx)\n  X_tr=X_train[train]\n  y_tr=to_categorical(y_train[train])\n  X_val=X_train[val]\n  y_val=to_categorical(y_train[val])\n\n  R=h_model.train_model(checkpoint_path, np.array(X_tr), np.array(y_tr), np.array(X_val), np.array(y_val), np.array(X_test), np.array(y_test),optimizer=tf.keras.optimizers.Adagrad(\n    learning_rate=0.001), epochs=20,batch_size=20)\n  scores.append(R)\n  del  X_tr, X_val, y_tr, y_val\n  #del h_model\n\nprint(scores)\nscores_df=pd.DataFrame(scores)\nscores_df.to_csv('/results/scores_AoS2.csv')\n#print(scores.mean())"},{"cell_type":"code","execution_count":null,"metadata":{"id":"iXBAiHyht8gH"},"outputs":[],"source":"accuracy=[]\nfor i in range (9):\n  accuracy.append(scores[i][1])\n"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B_iKPDa-usiF","outputId":"5e41b653-b3bb-497a-aff2-52311d2ae60a"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.46666667196485734\n"]}],"source":"mean_accuracy=np.mean(accuracy)\nprint(mean_accuracy)"},{"cell_type":"code","execution_count":null,"metadata":{"id":"mdb-pZzef4v_"},"outputs":[],"source":""}],"metadata":{"accelerator":"TPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}