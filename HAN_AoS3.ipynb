{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yVd7YbxbuwkB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/code/interpretable-han-for-document-classification-with-keras\n"
     ]
    }
   ],
   "source": [
    "cd /code/interpretable-han-for-document-classification-with-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/c2/e06851e8cc28dcad7c155f4753da8833ac06a5c704c109313b8d5a62968a/pip-23.2.1-py3-none-any.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 26.4MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 19.1.1\n",
      "    Uninstalling pip-19.1.1:\n",
      "      Successfully uninstalled pip-19.1.1\n",
      "Successfully installed pip-23.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.18\n",
      "  Downloading numpy-1.18.0-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.2\n",
      "    Uninstalling numpy-1.19.2:\n",
      "      Successfully uninstalled numpy-1.19.2\n",
      "Successfully installed numpy-1.18.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z7CC1CQSJoFw",
    "outputId": "0c32d940-30ee-4b70-d128-edda5442ca95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing hierachical_attention_network_for_document_classification.egg-info/PKG-INFO\n",
      "writing dependency_links to hierachical_attention_network_for_document_classification.egg-info/dependency_links.txt\n",
      "writing requirements to hierachical_attention_network_for_document_classification.egg-info/requires.txt\n",
      "writing top-level names to hierachical_attention_network_for_document_classification.egg-info/top_level.txt\n",
      "reading manifest file 'hierachical_attention_network_for_document_classification.egg-info/SOURCES.txt'\n",
      "writing manifest file 'hierachical_attention_network_for_document_classification.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "copying han/AoS3.py -> build/lib/han\n",
      "creating build/bdist.linux-x86_64\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/han\n",
      "copying build/lib/han/attention.py -> build/bdist.linux-x86_64/egg/han\n",
      "copying build/lib/han/selfattention_han_3_last.py -> build/bdist.linux-x86_64/egg/han\n",
      "copying build/lib/han/__init__.py -> build/bdist.linux-x86_64/egg/han\n",
      "copying build/lib/han/utils.py -> build/bdist.linux-x86_64/egg/han\n",
      "copying build/lib/han/AoS2.py -> build/bdist.linux-x86_64/egg/han\n",
      "copying build/lib/han/selfattention_HAN_3.py -> build/bdist.linux-x86_64/egg/han\n",
      "copying build/lib/han/model.py -> build/bdist.linux-x86_64/egg/han\n",
      "copying build/lib/han/AoS3.py -> build/bdist.linux-x86_64/egg/han\n",
      "copying build/lib/han/HBRNN.py -> build/bdist.linux-x86_64/egg/han\n",
      "byte-compiling build/bdist.linux-x86_64/egg/han/attention.py to attention.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/han/selfattention_han_3_last.py to selfattention_han_3_last.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/han/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/han/utils.py to utils.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/han/AoS2.py to AoS2.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/han/selfattention_HAN_3.py to selfattention_HAN_3.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/han/model.py to model.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/han/AoS3.py to AoS3.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/han/HBRNN.py to HBRNN.cpython-37.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying hierachical_attention_network_for_document_classification.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying hierachical_attention_network_for_document_classification.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying hierachical_attention_network_for_document_classification.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying hierachical_attention_network_for_document_classification.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying hierachical_attention_network_for_document_classification.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "creating 'dist/hierachical_attention_network_for_document_classification-0.1.0-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing hierachical_attention_network_for_document_classification-0.1.0-py3.7.egg\n",
      "Copying hierachical_attention_network_for_document_classification-0.1.0-py3.7.egg to /opt/conda/lib/python3.7/site-packages\n",
      "Adding hierachical-attention-network-for-document-classification 0.1.0 to easy-install.pth file\n",
      "\n",
      "Installed /opt/conda/lib/python3.7/site-packages/hierachical_attention_network_for_document_classification-0.1.0-py3.7.egg\n",
      "Processing dependencies for hierachical-attention-network-for-document-classification==0.1.0\n",
      "Searching for pandas==1.3.5\n",
      "Best match: pandas 1.3.5\n",
      "Adding pandas 1.3.5 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for Keras==2.4.3\n",
      "Best match: Keras 2.4.3\n",
      "Adding Keras 2.4.3 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for numpy==1.18.0\n",
      "Best match: numpy 1.18.0\n",
      "Adding numpy 1.18.0 to easy-install.pth file\n",
      "Installing f2py script to /opt/conda/bin\n",
      "Installing f2py3 script to /opt/conda/bin\n",
      "Installing f2py3.7 script to /opt/conda/bin\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for pytz==2022.7\n",
      "Best match: pytz 2022.7\n",
      "Adding pytz 2022.7 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for python-dateutil==2.8.2\n",
      "Best match: python-dateutil 2.8.2\n",
      "Adding python-dateutil 2.8.2 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for scipy==1.4.1\n",
      "Best match: scipy 1.4.1\n",
      "Adding scipy 1.4.1 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for PyYAML==5.3.1\n",
      "Best match: PyYAML 5.3.1\n",
      "Adding PyYAML 5.3.1 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for h5py==2.10.0\n",
      "Best match: h5py 2.10.0\n",
      "Adding h5py 2.10.0 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for six==1.12.0\n",
      "Best match: six 1.12.0\n",
      "Adding six 1.12.0 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Finished processing dependencies for hierachical-attention-network-for-document-classification==0.1.0\n"
     ]
    }
   ],
   "source": [
    "!python /code/interpretable-han-for-document-classification-with-keras/setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ThkIBuP54Gt7",
    "outputId": "a8a99e35-6038-4ae0-838a-14d84d7a6239"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-self-attention\n",
      "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from keras-self-attention) (1.18.0)\n",
      "Building wheels for collected packages: keras-self-attention\n",
      "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=fe4c4d983cc07eb873458c71d2aca5fa8624168d32266b32fb7dc4f9717e44ea\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
      "Successfully built keras-self-attention\n",
      "Installing collected packages: keras-self-attention\n",
      "Successfully installed keras-self-attention-0.51.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-18 14:36:20,262 - default - INFO - Pre-processsing data.\n",
      "2023-09-18 14:36:20,270 - default - INFO - Tokenization.\n",
      "2023-09-18 14:36:20,442 - default - INFO - Creating embedding matrix using pre-trained GloVe vectors.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code is for 3 level Hierarchichal Attention model\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "!pip install keras-self-attention\n",
    "\n",
    "from han.AoS3 import HAN\n",
    "\n",
    "\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Create a logger to provide info on the state of the\n",
    "# script\n",
    "stdout = logging.StreamHandler(sys.stdout)\n",
    "stdout.setFormatter(logging.Formatter(\n",
    "    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "))\n",
    "logger = logging.getLogger('default')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(stdout)\n",
    "\n",
    "\n",
    "max_doc_num = 5\n",
    "max_sent_length = 100\n",
    "max_sent_num = 25\n",
    "MAX_VOC_SIZE = 20000\n",
    "GLOVE_DIM = 200\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "Special_value=0\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# Pre processing                                    #\n",
    "#####################################################\n",
    "logger.info(\"Pre-processsing data.\")\n",
    "\n",
    "\n",
    "data = pd.read_csv('/data/Sample_data.csv', encoding='latin1')\n",
    "\n",
    "\n",
    "df=data.iloc[:,1:max_doc_num+1]\n",
    "reviews = data.iloc[:,1:max_doc_num+1].values\n",
    "target = data['sentiment'].values\n",
    "\n",
    "df['concat'] = pd.Series(df.fillna('').values.tolist()).map(lambda x: ''.join(map(str,x)))\n",
    "#del data\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# Tokenization                                      #\n",
    "#####################################################\n",
    "logger.info(\"Tokenization.\")\n",
    "\n",
    "# Build a Keras Tokenizer that can encode every token\n",
    "word_tokenizer = Tokenizer(num_words=MAX_VOC_SIZE)\n",
    "word_tokenizer.fit_on_texts(df['concat'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = np.zeros((len(data), max_doc_num , max_sent_num, max_sent_length), dtype='int32')\n",
    "\n",
    "#Tokenizing each patient interview to create embedding matrix\n",
    "\n",
    "for i, review in enumerate(reviews):\n",
    "  for j in range(len(review)):\n",
    "      if not pd.isnull(reviews[i,j]):\n",
    "        #word_tokenizer.fit_on_texts(review[j])\n",
    "        sentences = sent_tokenize(reviews[i,j])\n",
    "        tokenized_sentences = word_tokenizer.texts_to_sequences(\n",
    "          sentences\n",
    "        )\n",
    "        tokenized_sentences = pad_sequences(\n",
    "            tokenized_sentences, maxlen=max_sent_length\n",
    "        )\n",
    "\n",
    "        pad_size = max_sent_num - tokenized_sentences.shape[0]\n",
    "\n",
    "        if pad_size < 0:\n",
    "            tokenized_sentences = tokenized_sentences[0:max_sent_num]\n",
    "        else:\n",
    "            tokenized_sentences = np.pad(\n",
    "                tokenized_sentences, ((0,pad_size),(0,0)),\n",
    "                mode='constant', constant_values= Special_value\n",
    "            )\n",
    "      \n",
    "        X[i,j] = tokenized_sentences[None, ...]\n",
    "      else:\n",
    "        #X[i,j].fill(np.nan)\n",
    "        X[i,j]=0\n",
    "      \n",
    "\n",
    "# Transform the labels into a format Keras can handle\n",
    "y = to_categorical(target)\n",
    "\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# Word Embeddings                                   #\n",
    "#####################################################\n",
    "logger.info(\n",
    "    \"Creating embedding matrix using pre-trained GloVe vectors.\"\n",
    ")\n",
    "\n",
    "# Now, we need to build the embedding matrix. For this we use\n",
    "# a pretrained (on the wikipedia corpus) 100-dimensional GloVe\n",
    "# model.\n",
    "\n",
    "# Load the embeddings from a file\n",
    "embeddings = {}\n",
    "with open('/data/glove.6B.%dd.txt' % GLOVE_DIM, encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "        embeddings[word] = coefs\n",
    "\n",
    "# Initialize a matrix to hold the word embeddings\n",
    "embedding_matrix = np.random.random(\n",
    "    (len(word_tokenizer.word_index) + 1, GLOVE_DIM)\n",
    ")\n",
    "\n",
    "# Let the padded indices map to zero-vectors. This will\n",
    "# prevent the padding from influencing the results\n",
    "embedding_matrix[0] = 0\n",
    "\n",
    "# Loop though all the words in the word_index and where possible\n",
    "# replace the random initalization with the GloVe vector.\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WX4BthEBr5P0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 14:36:43.821067: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-18 14:36:43.844116: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499995000 Hz\n",
      "2023-09-18 14:36:43.844493: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f125b60df0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-18 14:36:43.844516: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "h_model = HAN(embedding_matrix, max_sent_length=100, max_sent_num=25, max_doc_num=5, word_embed_dim=100, sent_embed_dim=100, doc_embed_dim=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "uUMVbCFvdiTM"
   },
   "outputs": [],
   "source": [
    "checkpoint_path='/code/model.{epoch:02d}-{val_loss:.2f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NyiIKRJQaJ6z",
    "outputId": "42c98d84-5d34-45f5-ada9-cc18fd794d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 14:37:22.353111: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 3200000000 exceeds 10% of free system memory.\n",
      "2023-09-18 14:37:22.370231: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 3200000000 exceeds 10% of free system memory.\n",
      "2023-09-18 14:37:25.704247: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 3200000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6934 - acc: 0.6500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 14:37:28.728911: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 2880000000 exceeds 10% of free system memory.\n",
      "2023-09-18 14:37:28.729293: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 2880000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 0.6975 - acc: 0.5000\n",
      "Epoch 00001: val_loss improved from inf to 0.69464, saving model to /code/model.01-0.69.hdf5\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.6975 - acc: 0.5000 - val_loss: 0.6946 - val_acc: 0.6000\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6965 - acc: 0.5000\n",
      "Epoch 00002: val_loss did not improve from 0.69464\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6965 - acc: 0.5000 - val_loss: 0.6948 - val_acc: 0.6000\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6965 - acc: 0.5000\n",
      "Epoch 00003: val_loss did not improve from 0.69464\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6965 - acc: 0.5000 - val_loss: 0.6948 - val_acc: 0.6000\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6965 - acc: 0.5000\n",
      "Epoch 00004: val_loss did not improve from 0.69464\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6965 - acc: 0.5000 - val_loss: 0.6949 - val_acc: 0.6000\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6968 - acc: 0.2105\n",
      "Epoch 00005: val_loss did not improve from 0.69464\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6968 - acc: 0.2105 - val_loss: 0.6948 - val_acc: 0.6000\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6964 - acc: 0.5000\n",
      "Epoch 00006: val_loss did not improve from 0.69464\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6964 - acc: 0.5000 - val_loss: 0.6949 - val_acc: 0.6000\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6967 - acc: 0.2105\n",
      "Epoch 00007: val_loss did not improve from 0.69464\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6967 - acc: 0.2105 - val_loss: 0.6949 - val_acc: 0.6000\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6964 - acc: 0.5000\n",
      "Epoch 00008: val_loss did not improve from 0.69464\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6964 - acc: 0.5000 - val_loss: 0.6949 - val_acc: 0.6000\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6964 - acc: 0.5000\n",
      "Epoch 00009: val_loss did not improve from 0.69464\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6964 - acc: 0.5000 - val_loss: 0.6950 - val_acc: 0.6000\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6962 - acc: 0.5000\n",
      "Epoch 00010: val_loss did not improve from 0.69464\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6962 - acc: 0.5000 - val_loss: 0.6950 - val_acc: 0.6000\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6962 - acc: 0.5000\n",
      "Epoch 00011: val_loss did not improve from 0.69464\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6962 - acc: 0.5000 - val_loss: 0.6950 - val_acc: 0.6000\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6963 - acc: 0.5000\n",
      "Epoch 00012: val_loss did not improve from 0.69464\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6963 - acc: 0.5000 - val_loss: 0.6950 - val_acc: 0.6000\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6979 - acc: 0.1579\n",
      "Epoch 00013: val_loss did not improve from 0.69464\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6979 - acc: 0.1579 - val_loss: 0.6949 - val_acc: 0.6000\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6961 - acc: 0.5000\n",
      "Epoch 00014: val_loss did not improve from 0.69464\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6961 - acc: 0.5000 - val_loss: 0.6949 - val_acc: 0.6000\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6965 - acc: 0.2105\n",
      "Epoch 00015: val_loss did not improve from 0.69464\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6965 - acc: 0.2105 - val_loss: 0.6949 - val_acc: 0.6000\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6970 - acc: 0.1842\n",
      "Epoch 00016: val_loss did not improve from 0.69464\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6970 - acc: 0.1842 - val_loss: 0.6948 - val_acc: 0.6000\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6965 - acc: 0.2105\n",
      "Epoch 00017: val_loss did not improve from 0.69464\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6965 - acc: 0.2105 - val_loss: 0.6947 - val_acc: 0.6000\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6965 - acc: 0.2105\n",
      "Epoch 00018: val_loss did not improve from 0.69464\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6965 - acc: 0.2105 - val_loss: 0.6946 - val_acc: 0.6000\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6960 - acc: 0.5000\n",
      "Epoch 00019: val_loss did not improve from 0.69464\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6960 - acc: 0.5000 - val_loss: 0.6947 - val_acc: 0.6000\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6960 - acc: 0.5000\n",
      "Epoch 00020: val_loss did not improve from 0.69464\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6960 - acc: 0.5000 - val_loss: 0.6947 - val_acc: 0.6000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6973 - acc: 0.4000\n",
      "test loss, test acc: [0.6972857713699341, 0.4000000059604645]\n",
      "fold: 2\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6965 - acc: 0.2105\n",
      "Epoch 00001: val_loss improved from inf to 0.69467, saving model to /code/model.01-0.69.hdf5\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.6965 - acc: 0.2105 - val_loss: 0.6947 - val_acc: 0.6000\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6964 - acc: 0.2105\n",
      "Epoch 00002: val_loss improved from 0.69467 to 0.69461, saving model to /code/model.02-0.69.hdf5\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6964 - acc: 0.2105 - val_loss: 0.6946 - val_acc: 0.6000\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6960 - acc: 0.5000\n",
      "Epoch 00003: val_loss did not improve from 0.69461\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6960 - acc: 0.5000 - val_loss: 0.6947 - val_acc: 0.6000\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6970 - acc: 0.1842\n",
      "Epoch 00004: val_loss improved from 0.69461 to 0.69455, saving model to /code/model.04-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6970 - acc: 0.1842 - val_loss: 0.6946 - val_acc: 0.6000\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6960 - acc: 0.5000\n",
      "Epoch 00005: val_loss improved from 0.69455 to 0.69454, saving model to /code/model.05-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6960 - acc: 0.5000 - val_loss: 0.6945 - val_acc: 0.6000\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6959 - acc: 0.5000\n",
      "Epoch 00006: val_loss did not improve from 0.69454\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6959 - acc: 0.5000 - val_loss: 0.6946 - val_acc: 0.6000\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6963 - acc: 0.2105\n",
      "Epoch 00007: val_loss improved from 0.69454 to 0.69453, saving model to /code/model.07-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6963 - acc: 0.2105 - val_loss: 0.6945 - val_acc: 0.6000\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6960 - acc: 0.5000\n",
      "Epoch 00008: val_loss improved from 0.69453 to 0.69452, saving model to /code/model.08-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6960 - acc: 0.5000 - val_loss: 0.6945 - val_acc: 0.6000\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6968 - acc: 0.5000\n",
      "Epoch 00009: val_loss did not improve from 0.69452\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6968 - acc: 0.5000 - val_loss: 0.6947 - val_acc: 0.6000\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6968 - acc: 0.5000\n",
      "Epoch 00010: val_loss did not improve from 0.69452\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6968 - acc: 0.5000 - val_loss: 0.6949 - val_acc: 0.6000\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6962 - acc: 0.5000\n",
      "Epoch 00011: val_loss did not improve from 0.69452\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6962 - acc: 0.5000 - val_loss: 0.6950 - val_acc: 0.6000\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6962 - acc: 0.2105\n",
      "Epoch 00012: val_loss did not improve from 0.69452\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6962 - acc: 0.2105 - val_loss: 0.6949 - val_acc: 0.6000\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6959 - acc: 0.2368\n",
      "Epoch 00013: val_loss did not improve from 0.69452\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6959 - acc: 0.2368 - val_loss: 0.6949 - val_acc: 0.6000\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6961 - acc: 0.5000\n",
      "Epoch 00014: val_loss did not improve from 0.69452\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6961 - acc: 0.5000 - val_loss: 0.6950 - val_acc: 0.6000\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6961 - acc: 0.5000\n",
      "Epoch 00015: val_loss did not improve from 0.69452\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6961 - acc: 0.5000 - val_loss: 0.6951 - val_acc: 0.6000\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6961 - acc: 0.2105\n",
      "Epoch 00016: val_loss did not improve from 0.69452\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6961 - acc: 0.2105 - val_loss: 0.6950 - val_acc: 0.6000\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6961 - acc: 0.2105\n",
      "Epoch 00017: val_loss did not improve from 0.69452\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6961 - acc: 0.2105 - val_loss: 0.6950 - val_acc: 0.6000\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6961 - acc: 0.2105\n",
      "Epoch 00018: val_loss did not improve from 0.69452\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6961 - acc: 0.2105 - val_loss: 0.6949 - val_acc: 0.6000\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6957 - acc: 0.2368\n",
      "Epoch 00019: val_loss did not improve from 0.69452\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6957 - acc: 0.2368 - val_loss: 0.6949 - val_acc: 0.6000\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6956 - acc: 0.5000\n",
      "Epoch 00020: val_loss did not improve from 0.69452\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6956 - acc: 0.5000 - val_loss: 0.6949 - val_acc: 0.6000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6963 - acc: 0.4000\n",
      "test loss, test acc: [0.696308970451355, 0.4000000059604645]\n",
      "fold: 3\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6954 - acc: 0.5263\n",
      "Epoch 00001: val_loss improved from inf to 0.69675, saving model to /code/model.01-0.70.hdf5\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.6954 - acc: 0.5263 - val_loss: 0.6968 - val_acc: 0.4000\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6960 - acc: 0.2105\n",
      "Epoch 00002: val_loss did not improve from 0.69675\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6960 - acc: 0.2105 - val_loss: 0.6973 - val_acc: 0.4000\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6958 - acc: 0.5263\n",
      "Epoch 00003: val_loss did not improve from 0.69675\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6958 - acc: 0.5263 - val_loss: 0.6975 - val_acc: 0.4000\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6953 - acc: 0.5263\n",
      "Epoch 00004: val_loss did not improve from 0.69675\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6953 - acc: 0.5263 - val_loss: 0.6978 - val_acc: 0.4000\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6952 - acc: 0.5263\n",
      "Epoch 00005: val_loss did not improve from 0.69675\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6952 - acc: 0.5263 - val_loss: 0.6982 - val_acc: 0.4000\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6956 - acc: 0.5263\n",
      "Epoch 00006: val_loss did not improve from 0.69675\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6956 - acc: 0.5263 - val_loss: 0.6986 - val_acc: 0.4000\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6955 - acc: 0.5263\n",
      "Epoch 00007: val_loss did not improve from 0.69675\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6955 - acc: 0.5263 - val_loss: 0.6991 - val_acc: 0.4000\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6947 - acc: 0.5263\n",
      "Epoch 00008: val_loss did not improve from 0.69675\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6947 - acc: 0.5263 - val_loss: 0.6994 - val_acc: 0.4000\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6949 - acc: 0.5263\n",
      "Epoch 00009: val_loss did not improve from 0.69675\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6949 - acc: 0.5263 - val_loss: 0.6996 - val_acc: 0.4000\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6948 - acc: 0.5263\n",
      "Epoch 00010: val_loss did not improve from 0.69675\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6948 - acc: 0.5263 - val_loss: 0.6999 - val_acc: 0.4000\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6945 - acc: 0.5263\n",
      "Epoch 00011: val_loss did not improve from 0.69675\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6945 - acc: 0.5263 - val_loss: 0.7001 - val_acc: 0.4000\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6945 - acc: 0.5263\n",
      "Epoch 00012: val_loss did not improve from 0.69675\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6945 - acc: 0.5263 - val_loss: 0.7003 - val_acc: 0.4000\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6944 - acc: 0.5263\n",
      "Epoch 00013: val_loss did not improve from 0.69675\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6944 - acc: 0.5263 - val_loss: 0.7005 - val_acc: 0.4000\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6944 - acc: 0.5263\n",
      "Epoch 00014: val_loss did not improve from 0.69675\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6944 - acc: 0.5263 - val_loss: 0.7007 - val_acc: 0.4000\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6943 - acc: 0.5263\n",
      "Epoch 00015: val_loss did not improve from 0.69675\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6943 - acc: 0.5263 - val_loss: 0.7010 - val_acc: 0.4000\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6945 - acc: 0.5263\n",
      "Epoch 00016: val_loss did not improve from 0.69675\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6945 - acc: 0.5263 - val_loss: 0.7013 - val_acc: 0.4000\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6942 - acc: 0.5263\n",
      "Epoch 00017: val_loss did not improve from 0.69675\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6942 - acc: 0.5263 - val_loss: 0.7014 - val_acc: 0.4000\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6942 - acc: 0.5263\n",
      "Epoch 00018: val_loss did not improve from 0.69675\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6942 - acc: 0.5263 - val_loss: 0.7017 - val_acc: 0.4000\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6954 - acc: 0.5263\n",
      "Epoch 00019: val_loss did not improve from 0.69675\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6954 - acc: 0.5263 - val_loss: 0.7020 - val_acc: 0.4000\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6941 - acc: 0.5263\n",
      "Epoch 00020: val_loss did not improve from 0.69675\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6941 - acc: 0.5263 - val_loss: 0.7021 - val_acc: 0.4000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7021 - acc: 0.4000\n",
      "test loss, test acc: [0.7021499872207642, 0.4000000059604645]\n",
      "fold: 4\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6951 - acc: 0.5128\n",
      "Epoch 00001: val_loss improved from inf to 0.69572, saving model to /code/model.01-0.70.hdf5\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.6951 - acc: 0.5128 - val_loss: 0.6957 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6951 - acc: 0.5128\n",
      "Epoch 00002: val_loss improved from 0.69572 to 0.69570, saving model to /code/model.02-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6951 - acc: 0.5128 - val_loss: 0.6957 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6949 - acc: 0.5128\n",
      "Epoch 00003: val_loss improved from 0.69570 to 0.69567, saving model to /code/model.03-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6949 - acc: 0.5128 - val_loss: 0.6957 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6949 - acc: 0.5128\n",
      "Epoch 00004: val_loss improved from 0.69567 to 0.69564, saving model to /code/model.04-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6949 - acc: 0.5128 - val_loss: 0.6956 - val_acc: 0.5000\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6948 - acc: 0.5128\n",
      "Epoch 00005: val_loss improved from 0.69564 to 0.69561, saving model to /code/model.05-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6948 - acc: 0.5128 - val_loss: 0.6956 - val_acc: 0.5000\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6952 - acc: 0.5128\n",
      "Epoch 00006: val_loss improved from 0.69561 to 0.69557, saving model to /code/model.06-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6952 - acc: 0.5128 - val_loss: 0.6956 - val_acc: 0.5000\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6951 - acc: 0.5128\n",
      "Epoch 00007: val_loss improved from 0.69557 to 0.69554, saving model to /code/model.07-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6951 - acc: 0.5128 - val_loss: 0.6955 - val_acc: 0.5000\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6948 - acc: 0.5128\n",
      "Epoch 00008: val_loss improved from 0.69554 to 0.69551, saving model to /code/model.08-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6948 - acc: 0.5128 - val_loss: 0.6955 - val_acc: 0.5000\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6953 - acc: 0.5128\n",
      "Epoch 00009: val_loss improved from 0.69551 to 0.69549, saving model to /code/model.09-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6953 - acc: 0.5128 - val_loss: 0.6955 - val_acc: 0.5000\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6948 - acc: 0.5128\n",
      "Epoch 00010: val_loss improved from 0.69549 to 0.69546, saving model to /code/model.10-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6948 - acc: 0.5128 - val_loss: 0.6955 - val_acc: 0.5000\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6950 - acc: 0.5128\n",
      "Epoch 00011: val_loss improved from 0.69546 to 0.69543, saving model to /code/model.11-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6950 - acc: 0.5128 - val_loss: 0.6954 - val_acc: 0.5000\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6949 - acc: 0.5128\n",
      "Epoch 00012: val_loss improved from 0.69543 to 0.69541, saving model to /code/model.12-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6949 - acc: 0.5128 - val_loss: 0.6954 - val_acc: 0.5000\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6976 - acc: 0.5128\n",
      "Epoch 00013: val_loss improved from 0.69541 to 0.69540, saving model to /code/model.13-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6976 - acc: 0.5128 - val_loss: 0.6954 - val_acc: 0.5000\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6947 - acc: 0.5128\n",
      "Epoch 00014: val_loss improved from 0.69540 to 0.69538, saving model to /code/model.14-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6947 - acc: 0.5128 - val_loss: 0.6954 - val_acc: 0.5000\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6947 - acc: 0.5128\n",
      "Epoch 00015: val_loss improved from 0.69538 to 0.69535, saving model to /code/model.15-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6947 - acc: 0.5128 - val_loss: 0.6953 - val_acc: 0.5000\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6949 - acc: 0.5128\n",
      "Epoch 00016: val_loss improved from 0.69535 to 0.69532, saving model to /code/model.16-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6949 - acc: 0.5128 - val_loss: 0.6953 - val_acc: 0.5000\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6947 - acc: 0.5128\n",
      "Epoch 00017: val_loss improved from 0.69532 to 0.69530, saving model to /code/model.17-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6947 - acc: 0.5128 - val_loss: 0.6953 - val_acc: 0.5000\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6947 - acc: 0.5128\n",
      "Epoch 00018: val_loss improved from 0.69530 to 0.69528, saving model to /code/model.18-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6947 - acc: 0.5128 - val_loss: 0.6953 - val_acc: 0.5000\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6947 - acc: 0.5128\n",
      "Epoch 00019: val_loss improved from 0.69528 to 0.69526, saving model to /code/model.19-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6947 - acc: 0.5128 - val_loss: 0.6953 - val_acc: 0.5000\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6947 - acc: 0.5128\n",
      "Epoch 00020: val_loss improved from 0.69526 to 0.69524, saving model to /code/model.20-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6947 - acc: 0.5128 - val_loss: 0.6952 - val_acc: 0.5000\n",
      "1/1 [==============================] - 0s 967us/step - loss: 0.7009 - acc: 0.4000\n",
      "test loss, test acc: [0.7008992433547974, 0.4000000059604645]\n",
      "fold: 5\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6957 - acc: 0.5128\n",
      "Epoch 00001: val_loss improved from inf to 0.69523, saving model to /code/model.01-0.70.hdf5\n",
      "2/2 [==============================] - 9s 5s/step - loss: 0.6957 - acc: 0.5128 - val_loss: 0.6952 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6945 - acc: 0.5128\n",
      "Epoch 00002: val_loss improved from 0.69523 to 0.69520, saving model to /code/model.02-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6945 - acc: 0.5128 - val_loss: 0.6952 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6948 - acc: 0.5128\n",
      "Epoch 00003: val_loss improved from 0.69520 to 0.69517, saving model to /code/model.03-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6948 - acc: 0.5128 - val_loss: 0.6952 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6948 - acc: 0.5128\n",
      "Epoch 00004: val_loss improved from 0.69517 to 0.69514, saving model to /code/model.04-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6948 - acc: 0.5128 - val_loss: 0.6951 - val_acc: 0.5000\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6945 - acc: 0.5128\n",
      "Epoch 00005: val_loss improved from 0.69514 to 0.69512, saving model to /code/model.05-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6945 - acc: 0.5128 - val_loss: 0.6951 - val_acc: 0.5000\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6944 - acc: 0.5128\n",
      "Epoch 00006: val_loss improved from 0.69512 to 0.69509, saving model to /code/model.06-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6944 - acc: 0.5128 - val_loss: 0.6951 - val_acc: 0.5000\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6944 - acc: 0.5128\n",
      "Epoch 00007: val_loss improved from 0.69509 to 0.69507, saving model to /code/model.07-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6944 - acc: 0.5128 - val_loss: 0.6951 - val_acc: 0.5000\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6968 - acc: 0.5128\n",
      "Epoch 00008: val_loss improved from 0.69507 to 0.69504, saving model to /code/model.08-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6968 - acc: 0.5128 - val_loss: 0.6950 - val_acc: 0.5000\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6954 - acc: 0.5128\n",
      "Epoch 00009: val_loss improved from 0.69504 to 0.69503, saving model to /code/model.09-0.70.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6954 - acc: 0.5128 - val_loss: 0.6950 - val_acc: 0.5000\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6951 - acc: 0.5128\n",
      "Epoch 00010: val_loss improved from 0.69503 to 0.69499, saving model to /code/model.10-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6951 - acc: 0.5128 - val_loss: 0.6950 - val_acc: 0.5000\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6946 - acc: 0.5128\n",
      "Epoch 00011: val_loss improved from 0.69499 to 0.69497, saving model to /code/model.11-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6946 - acc: 0.5128 - val_loss: 0.6950 - val_acc: 0.5000\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6944 - acc: 0.5128\n",
      "Epoch 00012: val_loss improved from 0.69497 to 0.69495, saving model to /code/model.12-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6944 - acc: 0.5128 - val_loss: 0.6949 - val_acc: 0.5000\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6960 - acc: 0.5128\n",
      "Epoch 00013: val_loss improved from 0.69495 to 0.69494, saving model to /code/model.13-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6960 - acc: 0.5128 - val_loss: 0.6949 - val_acc: 0.5000\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6945 - acc: 0.5128\n",
      "Epoch 00014: val_loss improved from 0.69494 to 0.69491, saving model to /code/model.14-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6945 - acc: 0.5128 - val_loss: 0.6949 - val_acc: 0.5000\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6943 - acc: 0.5128\n",
      "Epoch 00015: val_loss improved from 0.69491 to 0.69489, saving model to /code/model.15-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6943 - acc: 0.5128 - val_loss: 0.6949 - val_acc: 0.5000\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6949 - acc: 0.5128\n",
      "Epoch 00016: val_loss improved from 0.69489 to 0.69486, saving model to /code/model.16-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6949 - acc: 0.5128 - val_loss: 0.6949 - val_acc: 0.5000\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6943 - acc: 0.5128\n",
      "Epoch 00017: val_loss improved from 0.69486 to 0.69484, saving model to /code/model.17-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6943 - acc: 0.5128 - val_loss: 0.6948 - val_acc: 0.5000\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6943 - acc: 0.5128\n",
      "Epoch 00018: val_loss improved from 0.69484 to 0.69482, saving model to /code/model.18-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6943 - acc: 0.5128 - val_loss: 0.6948 - val_acc: 0.5000\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6942 - acc: 0.5128\n",
      "Epoch 00019: val_loss improved from 0.69482 to 0.69480, saving model to /code/model.19-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6942 - acc: 0.5128 - val_loss: 0.6948 - val_acc: 0.5000\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6942 - acc: 0.5128\n",
      "Epoch 00020: val_loss improved from 0.69480 to 0.69478, saving model to /code/model.20-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6942 - acc: 0.5128 - val_loss: 0.6948 - val_acc: 0.5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6999 - acc: 0.4000\n",
      "test loss, test acc: [0.6999202966690063, 0.4000000059604645]\n",
      "fold: 6\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6945 - acc: 0.5128\n",
      "Epoch 00001: val_loss improved from inf to 0.69475, saving model to /code/model.01-0.69.hdf5\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.6945 - acc: 0.5128 - val_loss: 0.6948 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6944 - acc: 0.5128\n",
      "Epoch 00002: val_loss improved from 0.69475 to 0.69473, saving model to /code/model.02-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6944 - acc: 0.5128 - val_loss: 0.6947 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6944 - acc: 0.5128\n",
      "Epoch 00003: val_loss improved from 0.69473 to 0.69470, saving model to /code/model.03-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6944 - acc: 0.5128 - val_loss: 0.6947 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6941 - acc: 0.5128\n",
      "Epoch 00004: val_loss improved from 0.69470 to 0.69468, saving model to /code/model.04-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6941 - acc: 0.5128 - val_loss: 0.6947 - val_acc: 0.5000\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6940 - acc: 0.5128\n",
      "Epoch 00005: val_loss improved from 0.69468 to 0.69466, saving model to /code/model.05-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6940 - acc: 0.5128 - val_loss: 0.6947 - val_acc: 0.5000\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6944 - acc: 0.5128\n",
      "Epoch 00006: val_loss improved from 0.69466 to 0.69463, saving model to /code/model.06-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6944 - acc: 0.5128 - val_loss: 0.6946 - val_acc: 0.5000\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6940 - acc: 0.5128\n",
      "Epoch 00007: val_loss improved from 0.69463 to 0.69461, saving model to /code/model.07-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6940 - acc: 0.5128 - val_loss: 0.6946 - val_acc: 0.5000\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6942 - acc: 0.5128\n",
      "Epoch 00008: val_loss improved from 0.69461 to 0.69460, saving model to /code/model.08-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6942 - acc: 0.5128 - val_loss: 0.6946 - val_acc: 0.5000\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6940 - acc: 0.5128\n",
      "Epoch 00009: val_loss improved from 0.69460 to 0.69458, saving model to /code/model.09-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6940 - acc: 0.5128 - val_loss: 0.6946 - val_acc: 0.5000\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6945 - acc: 0.5128\n",
      "Epoch 00010: val_loss improved from 0.69458 to 0.69456, saving model to /code/model.10-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6945 - acc: 0.5128 - val_loss: 0.6946 - val_acc: 0.5000\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6951 - acc: 0.5128\n",
      "Epoch 00011: val_loss improved from 0.69456 to 0.69455, saving model to /code/model.11-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6951 - acc: 0.5128 - val_loss: 0.6946 - val_acc: 0.5000\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6939 - acc: 0.5128\n",
      "Epoch 00012: val_loss improved from 0.69455 to 0.69454, saving model to /code/model.12-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6939 - acc: 0.5128 - val_loss: 0.6945 - val_acc: 0.5000\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6942 - acc: 0.5128\n",
      "Epoch 00013: val_loss improved from 0.69454 to 0.69451, saving model to /code/model.13-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6942 - acc: 0.5128 - val_loss: 0.6945 - val_acc: 0.5000\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6939 - acc: 0.5128\n",
      "Epoch 00014: val_loss improved from 0.69451 to 0.69449, saving model to /code/model.14-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6939 - acc: 0.5128 - val_loss: 0.6945 - val_acc: 0.5000\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6944 - acc: 0.5128\n",
      "Epoch 00015: val_loss improved from 0.69449 to 0.69447, saving model to /code/model.15-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6944 - acc: 0.5128 - val_loss: 0.6945 - val_acc: 0.5000\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6938 - acc: 0.5128\n",
      "Epoch 00016: val_loss improved from 0.69447 to 0.69446, saving model to /code/model.16-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6938 - acc: 0.5128 - val_loss: 0.6945 - val_acc: 0.5000\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6939 - acc: 0.5128\n",
      "Epoch 00017: val_loss improved from 0.69446 to 0.69443, saving model to /code/model.17-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6939 - acc: 0.5128 - val_loss: 0.6944 - val_acc: 0.5000\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6940 - acc: 0.5128\n",
      "Epoch 00018: val_loss improved from 0.69443 to 0.69442, saving model to /code/model.18-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6940 - acc: 0.5128 - val_loss: 0.6944 - val_acc: 0.5000\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6939 - acc: 0.5128\n",
      "Epoch 00019: val_loss improved from 0.69442 to 0.69440, saving model to /code/model.19-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6939 - acc: 0.5128 - val_loss: 0.6944 - val_acc: 0.5000\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6945 - acc: 0.5128\n",
      "Epoch 00020: val_loss improved from 0.69440 to 0.69437, saving model to /code/model.20-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6945 - acc: 0.5128 - val_loss: 0.6944 - val_acc: 0.5000\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.6993 - acc: 0.4000\n",
      "test loss, test acc: [0.6993386745452881, 0.4000000059604645]\n",
      "fold: 7\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6937 - acc: 0.5128\n",
      "Epoch 00001: val_loss improved from inf to 0.69435, saving model to /code/model.01-0.69.hdf5\n",
      "2/2 [==============================] - 9s 5s/step - loss: 0.6937 - acc: 0.5128 - val_loss: 0.6944 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6938 - acc: 0.5128\n",
      "Epoch 00002: val_loss improved from 0.69435 to 0.69433, saving model to /code/model.02-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6938 - acc: 0.5128 - val_loss: 0.6943 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6962 - acc: 0.5128\n",
      "Epoch 00003: val_loss improved from 0.69433 to 0.69430, saving model to /code/model.03-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6962 - acc: 0.5128 - val_loss: 0.6943 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6937 - acc: 0.5128\n",
      "Epoch 00004: val_loss improved from 0.69430 to 0.69428, saving model to /code/model.04-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6937 - acc: 0.5128 - val_loss: 0.6943 - val_acc: 0.5000\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6937 - acc: 0.5128\n",
      "Epoch 00005: val_loss improved from 0.69428 to 0.69426, saving model to /code/model.05-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6937 - acc: 0.5128 - val_loss: 0.6943 - val_acc: 0.5000\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6937 - acc: 0.5128\n",
      "Epoch 00006: val_loss improved from 0.69426 to 0.69424, saving model to /code/model.06-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6937 - acc: 0.5128 - val_loss: 0.6942 - val_acc: 0.5000\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6936 - acc: 0.5128\n",
      "Epoch 00007: val_loss improved from 0.69424 to 0.69422, saving model to /code/model.07-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6936 - acc: 0.5128 - val_loss: 0.6942 - val_acc: 0.5000\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6939 - acc: 0.5128\n",
      "Epoch 00008: val_loss improved from 0.69422 to 0.69420, saving model to /code/model.08-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6939 - acc: 0.5128 - val_loss: 0.6942 - val_acc: 0.5000\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6936 - acc: 0.5128\n",
      "Epoch 00009: val_loss improved from 0.69420 to 0.69418, saving model to /code/model.09-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6936 - acc: 0.5128 - val_loss: 0.6942 - val_acc: 0.5000\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6936 - acc: 0.5128\n",
      "Epoch 00010: val_loss improved from 0.69418 to 0.69416, saving model to /code/model.10-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6936 - acc: 0.5128 - val_loss: 0.6942 - val_acc: 0.5000\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6935 - acc: 0.5128\n",
      "Epoch 00011: val_loss improved from 0.69416 to 0.69414, saving model to /code/model.11-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6935 - acc: 0.5128 - val_loss: 0.6941 - val_acc: 0.5000\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6935 - acc: 0.5128\n",
      "Epoch 00012: val_loss improved from 0.69414 to 0.69412, saving model to /code/model.12-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6935 - acc: 0.5128 - val_loss: 0.6941 - val_acc: 0.5000\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6935 - acc: 0.5128\n",
      "Epoch 00013: val_loss improved from 0.69412 to 0.69410, saving model to /code/model.13-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6935 - acc: 0.5128 - val_loss: 0.6941 - val_acc: 0.5000\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6946 - acc: 0.5128\n",
      "Epoch 00014: val_loss improved from 0.69410 to 0.69409, saving model to /code/model.14-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6946 - acc: 0.5128 - val_loss: 0.6941 - val_acc: 0.5000\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6935 - acc: 0.5128\n",
      "Epoch 00015: val_loss improved from 0.69409 to 0.69408, saving model to /code/model.15-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6935 - acc: 0.5128 - val_loss: 0.6941 - val_acc: 0.5000\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6938 - acc: 0.5128\n",
      "Epoch 00016: val_loss improved from 0.69408 to 0.69405, saving model to /code/model.16-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6938 - acc: 0.5128 - val_loss: 0.6941 - val_acc: 0.5000\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6935 - acc: 0.5128\n",
      "Epoch 00017: val_loss improved from 0.69405 to 0.69403, saving model to /code/model.17-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6935 - acc: 0.5128 - val_loss: 0.6940 - val_acc: 0.5000\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6942 - acc: 0.5128\n",
      "Epoch 00018: val_loss improved from 0.69403 to 0.69400, saving model to /code/model.18-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6942 - acc: 0.5128 - val_loss: 0.6940 - val_acc: 0.5000\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6941 - acc: 0.5128\n",
      "Epoch 00019: val_loss improved from 0.69400 to 0.69397, saving model to /code/model.19-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6941 - acc: 0.5128 - val_loss: 0.6940 - val_acc: 0.5000\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6934 - acc: 0.5128\n",
      "Epoch 00020: val_loss improved from 0.69397 to 0.69395, saving model to /code/model.20-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6934 - acc: 0.5128 - val_loss: 0.6940 - val_acc: 0.5000\n",
      "1/1 [==============================] - 0s 988us/step - loss: 0.6986 - acc: 0.4000\n",
      "test loss, test acc: [0.6986356973648071, 0.4000000059604645]\n",
      "fold: 8\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6934 - acc: 0.5128\n",
      "Epoch 00001: val_loss improved from inf to 0.69393, saving model to /code/model.01-0.69.hdf5\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.6934 - acc: 0.5128 - val_loss: 0.6939 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6937 - acc: 0.5128\n",
      "Epoch 00002: val_loss improved from 0.69393 to 0.69391, saving model to /code/model.02-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6937 - acc: 0.5128 - val_loss: 0.6939 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6933 - acc: 0.5128\n",
      "Epoch 00003: val_loss improved from 0.69391 to 0.69389, saving model to /code/model.03-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6933 - acc: 0.5128 - val_loss: 0.6939 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6935 - acc: 0.5128\n",
      "Epoch 00004: val_loss improved from 0.69389 to 0.69388, saving model to /code/model.04-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6935 - acc: 0.5128 - val_loss: 0.6939 - val_acc: 0.5000\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6938 - acc: 0.5128\n",
      "Epoch 00005: val_loss improved from 0.69388 to 0.69387, saving model to /code/model.05-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6938 - acc: 0.5128 - val_loss: 0.6939 - val_acc: 0.5000\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6933 - acc: 0.5128\n",
      "Epoch 00006: val_loss improved from 0.69387 to 0.69384, saving model to /code/model.06-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6933 - acc: 0.5128 - val_loss: 0.6938 - val_acc: 0.5000\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6934 - acc: 0.5128\n",
      "Epoch 00007: val_loss improved from 0.69384 to 0.69383, saving model to /code/model.07-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6934 - acc: 0.5128 - val_loss: 0.6938 - val_acc: 0.5000\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6935 - acc: 0.5128\n",
      "Epoch 00008: val_loss improved from 0.69383 to 0.69380, saving model to /code/model.08-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6935 - acc: 0.5128 - val_loss: 0.6938 - val_acc: 0.5000\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6934 - acc: 0.5128\n",
      "Epoch 00009: val_loss improved from 0.69380 to 0.69379, saving model to /code/model.09-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6934 - acc: 0.5128 - val_loss: 0.6938 - val_acc: 0.5000\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6943 - acc: 0.5128\n",
      "Epoch 00010: val_loss improved from 0.69379 to 0.69378, saving model to /code/model.10-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6943 - acc: 0.5128 - val_loss: 0.6938 - val_acc: 0.5000\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6933 - acc: 0.5128\n",
      "Epoch 00011: val_loss improved from 0.69378 to 0.69377, saving model to /code/model.11-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6933 - acc: 0.5128 - val_loss: 0.6938 - val_acc: 0.5000\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6933 - acc: 0.5128\n",
      "Epoch 00012: val_loss improved from 0.69377 to 0.69375, saving model to /code/model.12-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6933 - acc: 0.5128 - val_loss: 0.6938 - val_acc: 0.5000\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6933 - acc: 0.5128\n",
      "Epoch 00013: val_loss improved from 0.69375 to 0.69374, saving model to /code/model.13-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6933 - acc: 0.5128 - val_loss: 0.6937 - val_acc: 0.5000\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5128\n",
      "Epoch 00014: val_loss improved from 0.69374 to 0.69372, saving model to /code/model.14-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6932 - acc: 0.5128 - val_loss: 0.6937 - val_acc: 0.5000\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6936 - acc: 0.5128\n",
      "Epoch 00015: val_loss improved from 0.69372 to 0.69370, saving model to /code/model.15-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6936 - acc: 0.5128 - val_loss: 0.6937 - val_acc: 0.5000\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5128\n",
      "Epoch 00016: val_loss improved from 0.69370 to 0.69369, saving model to /code/model.16-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6932 - acc: 0.5128 - val_loss: 0.6937 - val_acc: 0.5000\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5128\n",
      "Epoch 00017: val_loss improved from 0.69369 to 0.69367, saving model to /code/model.17-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6932 - acc: 0.5128 - val_loss: 0.6937 - val_acc: 0.5000\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5128\n",
      "Epoch 00018: val_loss improved from 0.69367 to 0.69366, saving model to /code/model.18-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6932 - acc: 0.5128 - val_loss: 0.6937 - val_acc: 0.5000\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6941 - acc: 0.5128\n",
      "Epoch 00019: val_loss improved from 0.69366 to 0.69365, saving model to /code/model.19-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6941 - acc: 0.5128 - val_loss: 0.6936 - val_acc: 0.5000\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6933 - acc: 0.5128\n",
      "Epoch 00020: val_loss improved from 0.69365 to 0.69362, saving model to /code/model.20-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6933 - acc: 0.5128 - val_loss: 0.6936 - val_acc: 0.5000\n",
      "1/1 [==============================] - 0s 963us/step - loss: 0.6987 - acc: 0.4000\n",
      "test loss, test acc: [0.6987122893333435, 0.4000000059604645]\n",
      "fold: 9\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6930 - acc: 0.5128\n",
      "Epoch 00001: val_loss improved from inf to 0.69360, saving model to /code/model.01-0.69.hdf5\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.6930 - acc: 0.5128 - val_loss: 0.6936 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6933 - acc: 0.5128\n",
      "Epoch 00002: val_loss improved from 0.69360 to 0.69357, saving model to /code/model.02-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6933 - acc: 0.5128 - val_loss: 0.6936 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6938 - acc: 0.5128\n",
      "Epoch 00003: val_loss improved from 0.69357 to 0.69354, saving model to /code/model.03-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6938 - acc: 0.5128 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6935 - acc: 0.5128\n",
      "Epoch 00004: val_loss improved from 0.69354 to 0.69353, saving model to /code/model.04-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6935 - acc: 0.5128 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6937 - acc: 0.5128\n",
      "Epoch 00005: val_loss improved from 0.69353 to 0.69350, saving model to /code/model.05-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6937 - acc: 0.5128 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6930 - acc: 0.5128\n",
      "Epoch 00006: val_loss improved from 0.69350 to 0.69349, saving model to /code/model.06-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6930 - acc: 0.5128 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6929 - acc: 0.5128\n",
      "Epoch 00007: val_loss improved from 0.69349 to 0.69347, saving model to /code/model.07-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6929 - acc: 0.5128 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6928 - acc: 0.5128\n",
      "Epoch 00008: val_loss improved from 0.69347 to 0.69345, saving model to /code/model.08-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6928 - acc: 0.5128 - val_loss: 0.6934 - val_acc: 0.5000\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6928 - acc: 0.5128\n",
      "Epoch 00009: val_loss improved from 0.69345 to 0.69343, saving model to /code/model.09-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6928 - acc: 0.5128 - val_loss: 0.6934 - val_acc: 0.5000\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6930 - acc: 0.5128\n",
      "Epoch 00010: val_loss improved from 0.69343 to 0.69341, saving model to /code/model.10-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6930 - acc: 0.5128 - val_loss: 0.6934 - val_acc: 0.5000\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6933 - acc: 0.5128\n",
      "Epoch 00011: val_loss improved from 0.69341 to 0.69340, saving model to /code/model.11-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6933 - acc: 0.5128 - val_loss: 0.6934 - val_acc: 0.5000\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6933 - acc: 0.5128\n",
      "Epoch 00012: val_loss improved from 0.69340 to 0.69339, saving model to /code/model.12-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6933 - acc: 0.5128 - val_loss: 0.6934 - val_acc: 0.5000\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6938 - acc: 0.5128\n",
      "Epoch 00013: val_loss improved from 0.69339 to 0.69338, saving model to /code/model.13-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6938 - acc: 0.5128 - val_loss: 0.6934 - val_acc: 0.5000\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6930 - acc: 0.5128\n",
      "Epoch 00014: val_loss improved from 0.69338 to 0.69335, saving model to /code/model.14-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6930 - acc: 0.5128 - val_loss: 0.6934 - val_acc: 0.5000\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6927 - acc: 0.5128\n",
      "Epoch 00015: val_loss improved from 0.69335 to 0.69333, saving model to /code/model.15-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6927 - acc: 0.5128 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6927 - acc: 0.5128\n",
      "Epoch 00016: val_loss improved from 0.69333 to 0.69331, saving model to /code/model.16-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6927 - acc: 0.5128 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6927 - acc: 0.5128\n",
      "Epoch 00017: val_loss improved from 0.69331 to 0.69329, saving model to /code/model.17-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6927 - acc: 0.5128 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6928 - acc: 0.5128\n",
      "Epoch 00018: val_loss improved from 0.69329 to 0.69328, saving model to /code/model.18-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6928 - acc: 0.5128 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6927 - acc: 0.5128\n",
      "Epoch 00019: val_loss improved from 0.69328 to 0.69325, saving model to /code/model.19-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6927 - acc: 0.5128 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6926 - acc: 0.5128\n",
      "Epoch 00020: val_loss improved from 0.69325 to 0.69323, saving model to /code/model.20-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6926 - acc: 0.5128 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6983 - acc: 0.4000\n",
      "test loss, test acc: [0.6982882022857666, 0.4000000059604645]\n",
      "fold: 10\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6926 - acc: 0.5128\n",
      "Epoch 00001: val_loss improved from inf to 0.69321, saving model to /code/model.01-0.69.hdf5\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.6926 - acc: 0.5128 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6929 - acc: 0.5128\n",
      "Epoch 00002: val_loss improved from 0.69321 to 0.69318, saving model to /code/model.02-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6929 - acc: 0.5128 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6925 - acc: 0.5128\n",
      "Epoch 00003: val_loss improved from 0.69318 to 0.69316, saving model to /code/model.03-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6925 - acc: 0.5128 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6925 - acc: 0.5128\n",
      "Epoch 00004: val_loss improved from 0.69316 to 0.69315, saving model to /code/model.04-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6925 - acc: 0.5128 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6925 - acc: 0.5128\n",
      "Epoch 00005: val_loss improved from 0.69315 to 0.69313, saving model to /code/model.05-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6925 - acc: 0.5128 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6925 - acc: 0.5128\n",
      "Epoch 00006: val_loss improved from 0.69313 to 0.69310, saving model to /code/model.06-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6925 - acc: 0.5128 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6928 - acc: 0.5128\n",
      "Epoch 00007: val_loss improved from 0.69310 to 0.69308, saving model to /code/model.07-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6928 - acc: 0.5128 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6933 - acc: 0.5128\n",
      "Epoch 00008: val_loss improved from 0.69308 to 0.69305, saving model to /code/model.08-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6933 - acc: 0.5128 - val_loss: 0.6930 - val_acc: 0.5000\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6924 - acc: 0.5128\n",
      "Epoch 00009: val_loss improved from 0.69305 to 0.69303, saving model to /code/model.09-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6924 - acc: 0.5128 - val_loss: 0.6930 - val_acc: 0.5000\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6926 - acc: 0.5128\n",
      "Epoch 00010: val_loss improved from 0.69303 to 0.69302, saving model to /code/model.10-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6926 - acc: 0.5128 - val_loss: 0.6930 - val_acc: 0.5000\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6926 - acc: 0.5128\n",
      "Epoch 00011: val_loss improved from 0.69302 to 0.69300, saving model to /code/model.11-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6926 - acc: 0.5128 - val_loss: 0.6930 - val_acc: 0.5000\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6929 - acc: 0.5128\n",
      "Epoch 00012: val_loss improved from 0.69300 to 0.69299, saving model to /code/model.12-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6929 - acc: 0.5128 - val_loss: 0.6930 - val_acc: 0.5000\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6925 - acc: 0.5128\n",
      "Epoch 00013: val_loss improved from 0.69299 to 0.69298, saving model to /code/model.13-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6925 - acc: 0.5128 - val_loss: 0.6930 - val_acc: 0.5000\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6923 - acc: 0.5128\n",
      "Epoch 00014: val_loss improved from 0.69298 to 0.69296, saving model to /code/model.14-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6923 - acc: 0.5128 - val_loss: 0.6930 - val_acc: 0.5000\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6924 - acc: 0.5128\n",
      "Epoch 00015: val_loss improved from 0.69296 to 0.69293, saving model to /code/model.15-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6924 - acc: 0.5128 - val_loss: 0.6929 - val_acc: 0.5000\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6928 - acc: 0.5128\n",
      "Epoch 00016: val_loss improved from 0.69293 to 0.69292, saving model to /code/model.16-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6928 - acc: 0.5128 - val_loss: 0.6929 - val_acc: 0.5000\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6923 - acc: 0.5128\n",
      "Epoch 00017: val_loss improved from 0.69292 to 0.69290, saving model to /code/model.17-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6923 - acc: 0.5128 - val_loss: 0.6929 - val_acc: 0.5000\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6941 - acc: 0.5128\n",
      "Epoch 00018: val_loss improved from 0.69290 to 0.69289, saving model to /code/model.18-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6941 - acc: 0.5128 - val_loss: 0.6929 - val_acc: 0.5000\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6930 - acc: 0.5128\n",
      "Epoch 00019: val_loss improved from 0.69289 to 0.69286, saving model to /code/model.19-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6930 - acc: 0.5128 - val_loss: 0.6929 - val_acc: 0.5000\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6933 - acc: 0.5128\n",
      "Epoch 00020: val_loss improved from 0.69286 to 0.69285, saving model to /code/model.20-0.69.hdf5\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.6933 - acc: 0.5128 - val_loss: 0.6929 - val_acc: 0.5000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6979 - acc: 0.4000\n",
      "test loss, test acc: [0.6979226469993591, 0.4000000059604645]\n",
      "[[0.6972857713699341, 0.4000000059604645], [0.696308970451355, 0.4000000059604645], [0.7021499872207642, 0.4000000059604645], [0.7008992433547974, 0.4000000059604645], [0.6999202966690063, 0.4000000059604645], [0.6993386745452881, 0.4000000059604645], [0.6986356973648071, 0.4000000059604645], [0.6987122893333435, 0.4000000059604645], [0.6982882022857666, 0.4000000059604645], [0.6979226469993591, 0.4000000059604645]]\n"
     ]
    }
   ],
   "source": [
    "#####K-fold Cross Validation#####\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "TEST_SPLIT=0.1\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, target, test_size=TEST_SPLIT, random_state=0, stratify=y)\n",
    "\n",
    "\n",
    "y_test=to_categorical(y_test)\n",
    "\n",
    "kFold = StratifiedKFold(n_splits=10)\n",
    "scores = []\n",
    "idx = 0\n",
    "for train, val in kFold.split(X_train, y_train):\n",
    "  idx = idx+1\n",
    "  print(\"fold:\", idx)\n",
    "  X_tr=X_train[train]\n",
    "  y_tr=to_categorical(y_train[train])\n",
    "  X_val=X_train[val]\n",
    "  y_val=to_categorical(y_train[val])\n",
    "  \n",
    "  R=h_model.train_model(checkpoint_path, np.array(X_tr), np.array(y_tr), np.array(X_val), np.array(y_val), np.array(X_test), np.array(y_test),optimizer=tf.keras.optimizers.Adagrad(\n",
    "    learning_rate=0.001), epochs=20,batch_size=20)\n",
    "  scores.append(R)\n",
    "  del  X_tr, X_val, y_tr, y_val\n",
    "  #del h_model\n",
    "\n",
    "print(scores)\n",
    "scores_df=pd.DataFrame(scores)\n",
    "scores_df.to_csv('/results/scores_AoS3.csv')  \n",
    "#print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "iXBAiHyht8gH"
   },
   "outputs": [],
   "source": [
    "accuracy=[]\n",
    "for i in range (9):\n",
    "  accuracy.append(scores[i][1])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B_iKPDa-usiF",
    "outputId": "5e41b653-b3bb-497a-aff2-52311d2ae60a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4000000059604645\n"
     ]
    }
   ],
   "source": [
    "mean_accuracy=np.mean(accuracy)\n",
    "print(mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
